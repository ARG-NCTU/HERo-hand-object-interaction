{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from skimage.draw import polygon\n",
    "from skimage.feature import peak_local_max\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import os\n",
    "\n",
    "from scipy import ndimage\n",
    "import scipy.misc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets ,transforms\n",
    "import torchvision\n",
    "from matplotlib import cm\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HERo(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(HERo, self).__init__()\n",
    "        self.color_trunk = torchvision.models.resnet101(pretrained=True)\n",
    "        del self.color_trunk.fc, self.color_trunk.avgpool, self.color_trunk.layer4\n",
    "        self.depth_trunk = copy.deepcopy(self.color_trunk)\n",
    "        self.conv1 = nn.Conv2d(2048, 512, 1)\n",
    "        self.conv2 = nn.Conv2d(512, 128, 1)\n",
    "        self.conv3 = nn.Conv2d(128, n_classes, 1)\n",
    "    def forward(self, color, depth):\n",
    "        # Color\n",
    "        color_feat_1 = self.color_trunk.conv1(color) # 3 -> 64\n",
    "        color_feat_1 = self.color_trunk.bn1(color_feat_1)\n",
    "        color_feat_1 = self.color_trunk.relu(color_feat_1)\n",
    "        color_feat_1 = self.color_trunk.maxpool(color_feat_1) \n",
    "        color_feat_2 = self.color_trunk.layer1(color_feat_1) # 64 -> 256\n",
    "        color_feat_3 = self.color_trunk.layer2(color_feat_2) # 256 -> 512\n",
    "        color_feat_4 = self.color_trunk.layer3(color_feat_3) # 512 -> 1024\n",
    "        # Depth\n",
    "        depth_feat_1 = self.depth_trunk.conv1(depth) # 3 -> 64\n",
    "        depth_feat_1 = self.depth_trunk.bn1(depth_feat_1)\n",
    "        depth_feat_1 = self.depth_trunk.relu(depth_feat_1)\n",
    "        depth_feat_1 = self.depth_trunk.maxpool(depth_feat_1) \n",
    "        depth_feat_2 = self.depth_trunk.layer1(depth_feat_1) # 64 -> 256\n",
    "        depth_feat_3 = self.depth_trunk.layer2(depth_feat_2) # 256 -> 512\n",
    "        depth_feat_4 = self.depth_trunk.layer3(depth_feat_3) # 512 -> 1024\n",
    "        # Concatenate\n",
    "        feat = torch.cat([color_feat_4, depth_feat_4], dim=1) # 2048\n",
    "        feat_1 = self.conv1(feat)\n",
    "        feat_2 = self.conv2(feat_1)\n",
    "        feat_3 = self.conv3(feat_2)\n",
    "        return nn.Upsample(scale_factor=2, mode=\"bilinear\")(feat_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = HERo(3)\n",
    "net.load_state_dict(torch.load('/home/arg-medical/HERo-hand-object-interaction/models/3class_net_75.pth'))\n",
    "net = net.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "f = open('/home/arg-medical/HERo-hand-object-interaction/dataset/test.txt', \"r\")\n",
    "for i, line in enumerate(f):\n",
    "    name.append(line.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_net_mean = np.array([0.485, 0.456, 0.406])\n",
    "image_net_std  = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_list = []\n",
    "for i in range(16):\n",
    "    angle_list.append(i*22.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_tensor(idx_name):\n",
    "    color_o = cv2.imread('/home/arg-medical/HERo-hand-object-interaction/dataset/'+\"color/color\"+idx_name)\n",
    "    color_o = color_o[:,:,[2,1,0]]\n",
    "    depth_o = cv2.imread('/home/arg-medical/HERo-hand-object-interaction/dataset/'+\"depth/depth\"+idx_name, 0)\n",
    "    label = cv2.imread('/home/arg-medical/HERo-hand-object-interaction/dataset/'+\"label/label\"+idx_name)\n",
    "    center = (color_o.shape[0]/2, color_o.shape[1]/2)\n",
    "    Sample_angle = []\n",
    "    \n",
    "    for angle in angle_list:\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        color_img = cv2.warpAffine(color_o, M, (color_o.shape[0], color_o.shape[1]))\n",
    "        depth_img = cv2.warpAffine(depth_o, M, (color_o.shape[0], color_o.shape[1]))\n",
    "\n",
    "        # uint8 -> float\n",
    "        color = (color_img/255.).astype(float)\n",
    "        # BGR -> RGB and normalize\n",
    "        color_rgb = np.zeros(color.shape)\n",
    "        \n",
    "        for i in range(3):\n",
    "            color_rgb[:, :, i] = (color[:, :, 2-i]-image_net_mean[i])/image_net_std[i]\n",
    "            depth = (depth_img/1000.).astype(float) # to meters\n",
    "            \n",
    "            # SR300 depth range\n",
    "            depth = np.clip(depth, 0.0, 1.2)\n",
    "  \n",
    "            # Duplicate channel and normalize\n",
    "            depth_3c = np.zeros(color.shape)\n",
    "    \n",
    "        for i in range(3):\n",
    "            depth_3c[:, :, i] = (depth[:, :]-image_net_mean[i])/image_net_std[i]\n",
    "            # Unlabeled -> 2; unsuctionable -> 0; suctionable -> 1\n",
    "            transform = transforms.Compose([transforms.ToTensor(),])\n",
    "            color_tensor = transform(color_rgb).float()\n",
    "            depth_tensor = transform(depth_3c).float()\n",
    "    \n",
    "            sample = {\"color\": color_tensor, \"depth\": depth_tensor, \"origin_color\": color_img, \"origin_depth\": depth_img, \"label\": label}\n",
    "            Sample_angle.append(sample)\n",
    "            \n",
    "    return Sample_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_visualize(sample, name, angle, Net):\n",
    "    \n",
    "    # read rgb-d and depth image from dataloader\n",
    "    color_tensor = sample['color'].cuda()\n",
    "    depth_tensor = sample['depth'].cuda()\n",
    "    origin_color = sample['origin_color']\n",
    "    origin_depth = sample['origin_depth']\n",
    "    c, h, w = color_tensor.shape\n",
    "    color_tensor = color_tensor.reshape(1, c, h, w)\n",
    "    depth_tensor = depth_tensor.reshape(1, c, h, w)\n",
    "\n",
    "    # read original image (heightmap)\n",
    "    heightmap_c = origin_color\n",
    "    heightmap_c = cv2.resize(heightmap_c, (h, w))\n",
    "    heightmap_d = origin_depth\n",
    "    heightmap_d = cv2.resize(heightmap_d, (h, w))\n",
    "\n",
    "    # make prediction\n",
    "    predict = Net.forward(color_tensor, depth_tensor)\n",
    "\n",
    "\n",
    "    # process prediction result\n",
    "    graspable = predict[0, 1].detach().cpu().numpy()\n",
    "    graspable = cv2.resize(graspable, (h, w))\n",
    "    graspable[heightmap_d==0] = 0\n",
    "\n",
    "    graspable[graspable>=1] = 0.99999\n",
    "    graspable[graspable<0] = 0\n",
    "\n",
    "    graspable = cv2.GaussianBlur(graspable, (7, 7), 0)\n",
    "    affordanceMap = (graspable/np.max(graspable)*255).astype(np.uint8)\n",
    "    affordanceMap = cv2.applyColorMap(affordanceMap, cv2.COLORMAP_JET)\n",
    "    affordanceMap = affordanceMap[:,:,[2,1,0]]\n",
    "\n",
    "    # combine prediction result (heatmap) and original image (heightmap color image)\n",
    "    Result = heightmap_c[:,:,[2,1,0]] + affordanceMap\n",
    "    Result = cv2.addWeighted(heightmap_c, 0.7, affordanceMap, 0.3, 0)\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.subplot(131)\n",
    "    plt.title('Color image Angle : '+str(angle))\n",
    "    plt.imshow(heightmap_c)\n",
    "    plt.subplot(132)\n",
    "    plt.title('AffordanceMap')\n",
    "    plt.imshow(affordanceMap)\n",
    "    plt.subplot(133)\n",
    "    plt.title('Result')\n",
    "    plt.imshow(Result)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_pred(data, model):\n",
    "    angle_split = img_tensor(data)\n",
    "    i = 0\n",
    "    for ing in angle_split:\n",
    "        pred_visualize(ing, data, i*22.5, model)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_pred(name[1], net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-private",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
